<!DOCTYPE html><html lang="zh-HK" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Pytorch基础 | 边个濑椰のBlog</title><meta name="author" content="Kawauso"><meta name="copyright" content="Kawauso"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇是对Pytorch基础使用的学习，主要基于B站小土堆的https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1hE411t7RN&#x2F;以及Gemini的帮助，除实践外也包含了一些深度学习模型的原理  Dataset An abstract class representing a Dataset. All datasets that represent a map from keys">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch基础">
<meta property="og:url" content="https://ottercoconut.github.io/2026/01/22/pytorch/index.html">
<meta property="og:site_name" content="边个濑椰のBlog">
<meta property="og:description" content="本篇是对Pytorch基础使用的学习，主要基于B站小土堆的https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1hE411t7RN&#x2F;以及Gemini的帮助，除实践外也包含了一些深度学习模型的原理  Dataset An abstract class representing a Dataset. All datasets that represent a map from keys">
<meta property="og:locale" content="zh_HK">
<meta property="og:image" content="https://ottercoconut.github.io/img/avatar.gif">
<meta property="article:published_time" content="2026-01-22T02:49:28.000Z">
<meta property="article:modified_time" content="2026-02-01T05:46:18.501Z">
<meta property="article:author" content="Kawauso">
<meta property="article:tag" content="Dataset">
<meta property="article:tag" content="TensorBoard">
<meta property="article:tag" content="Transforms">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ottercoconut.github.io/img/avatar.gif"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch基础",
  "url": "https://ottercoconut.github.io/2026/01/22/pytorch/",
  "image": "https://ottercoconut.github.io/img/avatar.gif",
  "datePublished": "2026-01-22T02:49:28.000Z",
  "dateModified": "2026-02-01T05:46:18.501Z",
  "author": [
    {
      "@type": "Person",
      "name": "Kawauso",
      "url": "https://ottercoconut.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ottercoconut.github.io/2026/01/22/pytorch/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '複製成功',
    error: '複製失敗',
    noSupport: '瀏覽器不支援'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '剛剛',
    min: '分鐘前',
    hour: '小時前',
    day: '天前',
    month: '個月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加載更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch基础',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><style>
  /* 统一样式：圆角、半透明、过渡 */
  #aside-content .card-widget {
    border-radius: 15px !important;
    border: 1px solid rgba(255, 255, 255, 0.5); /* 增加一层淡白色边框，增加高级感 */
    transition: transform 0.3s;
  }

  /* 鼠标悬停时轻微放大，更有互动感 */
  #aside-content .card-widget:hover {
    transform: translateY(-5px);
  }

  /* 1. 红色系（淡樱花粉） */
  #aside-content .card-widget:nth-child(1) { background-color: rgba(255, 182, 193, 0.4) !important; }
  
  /* 2. 橙色系（淡杏色） */
  #aside-content .card-widget:nth-child(2) { background-color: rgba(255, 218, 185, 0.4) !important; }
  
  /* 3. 黄色系（淡奶油黄） */
  #aside-content .card-widget:nth-child(3) { background-color: rgba(255, 250, 205, 0.4) !important; }
  
  /* 4. 绿色系（薄荷绿） */
  #aside-content .card-widget:nth-child(4) { background-color: rgba(152, 251, 152, 0.3) !important; }
  
  /* 5. 蓝色系（淡天蓝） */
  #aside-content .card-widget:nth-child(5) { background-color: rgba(175, 238, 238, 0.4) !important; }
  
  /* 6. 紫色系（薰衣草紫） */
  #aside-content .card-widget:nth-child(6) { background-color: rgba(230, 230, 250, 0.5) !important; }
</style>
<meta name="generator" content="Hexo 8.1.1"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">边个濑椰のBlog</span></a><a class="nav-page-title" href="/"><span class="site-name">Pytorch基础</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首頁</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Pytorch基础</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">發表於</span><time class="post-meta-date-created" datetime="2026-01-22T02:49:28.000Z" title="發表於 2026-01-22 10:49:28">2026-01-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新於</span><time class="post-meta-date-updated" datetime="2026-02-01T05:46:18.501Z" title="更新於 2026-02-01 13:46:18">2026-02-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">瀏覽量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>本篇是对Pytorch基础使用的学习，主要基于B站小土堆的<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hE411t7RN/%E4%BB%A5%E5%8F%8AGemini%E7%9A%84%E5%B8%AE%E5%8A%A9%EF%BC%8C%E9%99%A4%E5%AE%9E%E8%B7%B5%E5%A4%96%E4%B9%9F%E5%8C%85%E5%90%AB%E4%BA%86%E4%B8%80%E4%BA%9B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8E%9F%E7%90%86">https://www.bilibili.com/video/BV1hE411t7RN/以及Gemini的帮助，除实践外也包含了一些深度学习模型的原理</a></p>
<h2 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset</h2>
<p>An abstract class representing a <code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite <code>__getitem__</code>, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite <code>__len__</code>, which is expected to return the size of the dataset by many <code>~torch.utils.data.Sampler</code> implementations and the default options of <code>~torch.utils.data.DataLoader</code>. Subclasses could also optionally implement <code>__getitems__</code>, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.</p>
<p><strong>note</strong></p>
<ul>
<li><code>~torch.utils.data.DataLoader</code> by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.</li>
</ul>
<blockquote>
<p>一个表示数据集的抽象类。</p>
<p>所有实现从键（keys）到数据样本（data samples）映射的数据集都应该继承这个类。所有子类都必须重写 <code>__getitem__</code> 方法，以支持根据给定的键获取数据样本。子类也可以选择性地重写 <code>__len__</code> 方法，许多 <code>~torch.utils.data.Sampler</code> 实现和 <code>~torch.utils.data.DataLoader</code> 的默认选项都需要通过这个方法返回数据集的大小。子类还可以选择性地实现 <code>__getitems__</code> 方法，以加速批量样本的加载。此方法接受批处理样本的索引列表，并返回样本列表。</p>
<p>注意</p>
<p>默认情况下，<code>~torch.utils.data.DataLoader</code> 会构造一个生成整数索引的采样器。要让其与非整数索引/键的映射式数据集一起工作，必须提供自定义的采样器。</p>
</blockquote>
<h3 id="自定义dataset"><a class="markdownIt-Anchor" href="#自定义dataset"></a> 自定义Dataset</h3>
<p>那么在实际的使用中，比如有一个数据集（我们拿之前用过的fer2013来举例）：</p>
<ul>
<li>
<p>先定义一个数据类继承<code>Dataset</code>，定义<code>__init__()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        <span class="variable language_">self</span>.root_dir = root_dir <span class="comment"># 如 fer2013/test</span></span><br><span class="line">        <span class="variable language_">self</span>.label_dir = label_dir <span class="comment"># 如 angry</span></span><br><span class="line">        <span class="variable language_">self</span>.path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.label_dir) <span class="comment"># 路径合并函数 解决跨系统的文件路径问题</span></span><br><span class="line">        <span class="variable language_">self</span>.img_path = os.listdir(<span class="variable language_">self</span>.path) <span class="comment"># 如angry文件夹下，所有图片名的string list</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>重写<code>__getitem()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        img_name = <span class="variable language_">self</span>.img_path[index]</span><br><span class="line">        img_item_path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.label_dir, img_name) <span class="comment"># 拼接出具体的某一个图片的路径</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        label = <span class="variable language_">self</span>.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label <span class="comment"># 返回数据集中的一个对象（图片）及其类型</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>重写<code>__len()__</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_path)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>定义示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">root_dir = <span class="string">&quot;fer2013/train&quot;</span></span><br><span class="line">angry_label_dir = <span class="string">&quot;angry&quot;</span></span><br><span class="line">angry_dataset = MyData(root_dir, angry_label_dir)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Dataset合并</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">disguest_label_dir = <span class="string">&quot;disguest&quot;</span></span><br><span class="line">disgust_dataset = MyData(root_dir, disguest_label_dir)</span><br><span class="line"></span><br><span class="line">train_dataset = angry_dataset + disgust_dataset <span class="comment"># &#x27;+&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr />
<h2 id="tensorboard"><a class="markdownIt-Anchor" href="#tensorboard"></a> TensorBoard</h2>
<h3 id="summarywriter"><a class="markdownIt-Anchor" href="#summarywriter"></a> SummaryWriter</h3>
<ul>
<li>创建日志目录</li>
<li>初始化<code>Writer</code>对象</li>
<li>生成事件文件
<ul>
<li><strong>这个文件才是真正的数据库</strong>(events.out…)当后续调用 <code>writer.add_scalar</code> 时，数据并不是直接画在屏幕上，而是被追加写进这个文件里。</li>
<li>当你执行完相应代码，运行 <code>tensorboard --logdir=logs</code> 时，TensorBoard 的后端服务器就会去读取这个文件里的内容，并在浏览器里渲染成图表。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer  = SummaryWriter(<span class="string">&quot;logs&quot;</span>) <span class="comment"># 这行代码会在你的项目根目录下创建一个名为&quot;logs&quot;的文件夹</span></span><br></pre></td></tr></table></figure>
<p>需注意，每次实验可以用<strong>不同的文件夹来记录数据</strong>，比如<code>writer = SummaryWriter(&quot;logs/lr0.01_batch32&quot;)</code>修改了学习率后<code>writer = SummaryWriter(&quot;logs/lr0.001_batch32&quot;)</code></p>
<h4 id="add_scalar"><a class="markdownIt-Anchor" href="#add_scalar"></a> add_scalar()</h4>
<ul>
<li>绘图函数<code>add_scalar()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(method) <span class="keyword">def</span> <span class="title function_">add_scalar</span>(<span class="params"></span></span><br><span class="line"><span class="params">    tag: <span class="type">Any</span>,</span></span><br><span class="line"><span class="params">    scalar_value: <span class="type">Any</span>, <span class="comment"># 对应图像的y轴</span></span></span><br><span class="line"><span class="params">    global_step: <span class="type">Any</span> | <span class="literal">None</span> = <span class="literal">None</span>, <span class="comment"># 对应图像的x轴</span></span></span><br><span class="line"><span class="params">    walltime: <span class="type">Any</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    new_style: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    double_precision: <span class="built_in">bool</span> = <span class="literal">False</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">​``` 例 ```</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=2x&quot;</span>, <span class="number">2</span>*i, i)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h4 id="add_image"><a class="markdownIt-Anchor" href="#add_image"></a> add_image()</h4>
<ul>
<li>查看图片函数<code>add_image()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(method) <span class="keyword">def</span> <span class="title function_">add_image</span>(<span class="params"></span></span><br><span class="line"><span class="params">    tag: <span class="type">Any</span>,</span></span><br><span class="line"><span class="params">    img_tensor: <span class="type">Any</span>,</span></span><br><span class="line"><span class="params">    global_step: <span class="type">Any</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    walltime: <span class="type">Any</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dataformats: <span class="built_in">str</span> = <span class="string">&quot;CHW&quot;</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>要注意参数的类型要求：<code>img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data</code>，所以要将PIL类型的图片进行转换，比如用<code>numpy</code>来转换。以及数据的shape要求： Tensor with :math:<code>(1, H, W)</code>, :math:<code>(H, W)</code>, :math:<code>(H, W, 3)</code> is also suitable as long as corresponding <code>dataformats</code> argument is passed, e.g. <code>CHW</code>, <code>HWC</code>, <code>HW</code>. 即三种图片数据的格式，其<strong>通道数、高度、宽度</strong>的顺序不同。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">img_array = np.array(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;图像形状: <span class="subst">&#123;img_array.shape&#125;</span>&quot;</span>)</span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>,img_array,<span class="number">1</span>,dataformats=<span class="string">&#x27;HW&#x27;</span>) <span class="comment"># 由.shape可得</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h4 id="add_graph"><a class="markdownIt-Anchor" href="#add_graph"></a> add_graph()</h4>
<hr />
<h2 id="常见的transforms"><a class="markdownIt-Anchor" href="#常见的transforms"></a> 常见的Transforms</h2>
<p>下面基本都是对图像的处理方法</p>
<h3 id="totensor"><a class="markdownIt-Anchor" href="#totensor"></a> ToTensor</h3>
<p>Convert a PIL Image or ndarray to tensor and scale the values accordingly.</p>
<p>This transform does not support torchscript.</p>
<p>Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8</p>
<p>In the other cases, tensors are returned without scaling.</p>
<p><strong>note</strong></p>
<ul>
<li>
<p>Because the input image is scaled to [0.0, 1.0], this transformation should not be used when transforming target image masks. See the [references](vscode-file://vscode-app/d:/Microsoft VS Code/resources/app/out/vs/code/electron-browser/workbench/workbench.html) for implementing the transforms for image masks.</p>
</li>
<li>
<p><strong>将PIL图片转为torch类型</strong>，如<code>torch.Size([1, 48, 48])</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trans_totensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_totensor(img)</span><br><span class="line"><span class="built_in">print</span>(img_tensor.shape)</span><br><span class="line">writer.add_image(<span class="string">&quot;ToTensor&quot;</span>,img_tensor)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h3 id="normalize"><a class="markdownIt-Anchor" href="#normalize"></a> Normalize</h3>
<p>Normalize a tensor image with mean and standard deviation.</p>
<p>​    This transform does not support PIL Image.</p>
<p>​    Given mean: <code>(mean[1],...,mean[n])</code> and std: <code>(std[1],..,std[n])</code> for <code>n</code></p>
<p>​    channels, this transform will normalize each channel of the input</p>
<p>​    <code>torch.*Tensor</code> i.e.,</p>
<p>​    <code>output[channel] = (input[channel] - mean[channel]) / std[channel]</code></p>
<p>note:</p>
<p>​        This transform acts out of place, i.e., it does not mutate the input tensor.</p>
<p>Args:</p>
<p>​        mean (sequence): Sequence of means for each channel.</p>
<p>​        std (sequence): Sequence of standard deviations for each channel.</p>
<p>​        inplace(bool,optional): Bool to make this operation in-place.</p>
<ul>
<li><code>Normailize</code> 的数学原理是：</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mfrac><mrow><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi></mrow><mrow><mi>s</mi><mi>t</mi><mi>d</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">output = \frac{input-mean}{std}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.02252em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3365200000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">d</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li>参数<code>mean</code>影响的是“中心位置”，在<code>ToTensor</code>之后，像素值在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 之间，中心大约在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn></mrow><annotation encoding="application/x-tex">0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span>，如<code>mean = 0.5</code>，那么减去 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn></mrow><annotation encoding="application/x-tex">0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span> 后，数据的中心变成了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>。原本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 的范围变成了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>0.5</mn><mo separator="true">,</mo><mn>0.5</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-0.5, 0.5]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mclose">]</span></span></span></span></li>
<li>参数<code>std</code>影响的是“缩放幅度”，如<code>std = 0.5</code>，那么就是将数据范围再除以<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn></mrow><annotation encoding="application/x-tex">0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span>，最终范围从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>0.5</mn><mo separator="true">,</mo><mn>0.5</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-0.5, 0.5]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mclose">]</span></span></span></span> 变成了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></li>
</ul>
<h3 id="resize"><a class="markdownIt-Anchor" href="#resize"></a> Resize</h3>
<p>Resize the input image to the given size.</p>
<p>​    If the image is torch Tensor, it is expected</p>
<p>​    to have […, H, W] shape, where … means a maximum of two leading     dimensions</p>
<p>Args:</p>
<p>​        size (sequence or int): Desired output size. If size is a sequence like</p>
<p>​        (h, w), output size will be matched to this. If size is an int,</p>
<p>​        smaller edge of the image will be matched to this number.</p>
<p>​        i.e, if height &gt; width, then image will be rescaled to</p>
<p>​        (size * height / width, size).</p>
<ul>
<li><code>resize()</code>支持PIL和Tensor两种图片格式，如果是 Tensor，期望形状为 <code>[..., H, W]</code>。这里的 <code>...</code> 表示它可以处理 <code>[C, H, W]</code>（单张图）或 <code>[B, C, H, W]</code>（一个 Batch 的图）</li>
<li>参数<code>size</code>需注意写成序列形式，如<code>resize((512, 512))</code>，而如果只输入一个参数，如<code>resize(512)</code>，那么图片短边会变为512，而长边会按比例改变</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(img.size)</span><br><span class="line">trans_resize = transforms.Resize((<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line">img_resize = trans_resize(img)</span><br><span class="line"><span class="built_in">print</span>(img_resize)</span><br></pre></td></tr></table></figure>
<h3 id="compse"><a class="markdownIt-Anchor" href="#compse"></a> Compse</h3>
<p>Composes several transforms together. This transform does not support torchscript.</p>
<p>​    Please, see the note below.</p>
<p>Args:</p>
<p>​    transforms (list of <code>Transform</code> objects): list of transforms to compose.</p>
<ul>
<li><code>compse()</code>操作是对各种<code>transforms</code>操作的流水线<strong>类</strong></li>
<li>在深度学习中，图片通常需要经过一系列的固定步骤（比如：缩放 -&gt; 转为 Tensor -&gt; 归一化）。如果不用 <code>Compose</code>，你每一张图都要手动调用多个函数，代码会非常冗余</li>
<li><code>compse()</code>的参数类型是一个列表，操作是按顺序执行的，所以前一个操作输出的数据类型必须能作为下一个操作的输入。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="comment"># 定义训练集的预处理</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),           <span class="comment"># VGG16标准输入是224x224</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),       <span class="comment"># 数据增强：随机水平翻转</span></span><br><span class="line">    transforms.ToTensor(),                   <span class="comment"># 归一化到 [0.0, 1.0]</span></span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>])       <span class="comment"># 标准化到 [-1.0, 1.0]</span></span><br><span class="line">])</span><br><span class="line">img_tensor = train_transform(img)</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="pytorch数据集使用"><a class="markdownIt-Anchor" href="#pytorch数据集使用"></a> Pytorch数据集使用</h2>
<p>例如，我们要导入视觉学习的数据集，可以直接在程序中进行数据集的下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset...&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset...&quot;</span>, train=false, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>root</code>参数表示数据集存放的位置，<code>train</code>参数表示数据集是否用来训练，<code>download</code>参数表示是否下载到本地（会生成一个下载链接）</li>
<li>具体的参数设置，每个数据集都可能有所区别…</li>
<li>如果是下载完成到本地的数据集，可以将其复制到项目目录的dataset文件夹下，再运行程序即可节省下载的时间</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(test_set.classes) <span class="comment"># 可以看到测试数据集的所有类别</span></span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target]) <span class="comment"># 输出测试集第一个元素对应的类别</span></span><br></pre></td></tr></table></figure>
<hr />
<h2 id="dataloader"><a class="markdownIt-Anchor" href="#dataloader"></a> DataLoader</h2>
<p>Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.</p>
<p>​    The :class:<code>~torch.utils.data.DataLoader</code> supports both map-style and</p>
<p>​    iterable-style datasets with single- or multi-process loading, customizing</p>
<p>​    loading order and optional automatic batching (collation) and memory pinning.</p>
<p>在训练模型时，不能一次性把数据集中的大量数据塞进内存。<code>DataLoader</code>实现了：</p>
<ul>
<li><strong>Batching（批处理）：</strong> 把图片打包成一组组（Batch）。</li>
<li><strong>Shuffling（打乱）：</strong> 每一轮训练（Epoch）开始时随机洗牌，防止模型死记硬背数据顺序。</li>
<li><strong>Parallel Computing（并行加载）：</strong> 利用多核 CPU 提前准备下一批数据，不让 GPU 等待。</li>
</ul>
<table>
<thead>
<tr>
<th><strong>参数名</strong></th>
<th><strong>常用取值</strong></th>
<th><strong>作用描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>dataset</strong></td>
<td>自定义 Dataset</td>
<td>必填。告诉 DataLoader 从哪个“仓库”取数据。</td>
</tr>
<tr>
<td><strong>batch_size</strong></td>
<td>16, 32, 64…</td>
<td><strong>每批装载的数量。</strong> 越大训练越快，但越占显存。在 FER 表情识别中，32 或 64 是常用数值。</td>
</tr>
<tr>
<td><strong>shuffle</strong></td>
<td><code>True</code> / <code>False</code></td>
<td><strong>是否打乱顺序。</strong> 训练集通常设为 <code>True</code>（增加随机性）；测试集通常设为 <code>False</code>。</td>
</tr>
<tr>
<td><strong>num_workers</strong></td>
<td>0, 2, 4, 8…</td>
<td><strong>多进程加载。</strong> <code>0</code> 表示只用主进程（慢）。增加数值可以加快读取速度。<strong>建议：</strong> 设为 CPU 核心数的一半。</td>
</tr>
<tr>
<td><strong>drop_last</strong></td>
<td><code>True</code> / <code>False</code></td>
<td><strong>丢弃最后多余的数据。</strong> 比如有 100 张图，<code>batch_size=32</code>。最后剩 4 张不够一包，设为 <code>True</code> 就会把这 4 张扔掉，确保每个 Batch 大小一致。</td>
</tr>
<tr>
<td><strong>pin_memory</strong></td>
<td><code>True</code></td>
<td><strong>内存锁页。</strong> 如果你用 GPU 训练，设为 <code>True</code> 可以加快数据从内存传输到显存的速度。</td>
</tr>
</tbody>
</table>
<ul>
<li>例如，我们用<code>DataLoader</code>处理CIFAR10中的数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>结合循环和<code>tensorboard</code>，输出每一个<code>epoch</code>中每一<code>step</code>用到的图片</li>
<li>下面代码中<code>step + epoch * len(test_loader)</code>是使用了全局步长，也可以不这样写，而是把每个<code>epoch</code>来当作一组</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        writer.add_images(<span class="string">&quot;test_data_batch&quot;</span>, imgs, step + epoch * <span class="built_in">len</span>(test_loader))</span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="nnmodule"><a class="markdownIt-Anchor" href="#nnmodule"></a> nn.Module</h2>
<p>Base class for all neural network modules.</p>
<p>​    Your models should also subclass this class.</p>
<p>​    Modules can also contain other Modules, allowing to nest them in a tree structure.</p>
<ul>
<li>在 PyTorch 中，无论是简单的线性层，还是复杂的 VGG16 或 Transformer，本质上都是一个 <code>nn.Module</code>。它是所有神经网络模块的<strong>基类</strong></li>
<li><code>nn.Module</code> 支持嵌套，当你对大模型调用 <code>model.to(&quot;cuda&quot;)</code> 时，PyTorch 会顺着这棵“树”，自动把里面所有的子层都搬到 GPU 上。</li>
<li>只要在 <code>__init__</code> 中把一个层赋值给 <code>self.xxx</code>，PyTorch 就会自动识别出其中的<strong>权重 (Weights)</strong> 和 <strong>偏置 (Bias)</strong>，并将它们加入到待优化的参数列表中。</li>
</ul>
<p>编写一个<code>nn.Module</code>子类时，必须重写<code>__init()__</code>和<code>forward()</code></p>
<ul>
<li><code>__init(self)__</code>
<ul>
<li>在这里定义网络层（卷积、池化、全连接等）。</li>
<li>必须调用 <code>super().__init__()</code>。这行代码的作用是初始化父类的属性，如果没有它，PyTorch 就没法自动追踪定义的层，模型也就无法训练。</li>
</ul>
</li>
<li><code>forward(self, x)</code>
<ul>
<li>定义数据的流向。图片进去后，先过哪一层，再过哪一层。</li>
<li>不需要手动调用 <code>forward</code>，只需要运行 <code>model(input)</code>，PyTorch 会自动触发 <code>forward</code>。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span> <span class="comment"># 简单地将输出 +1 再输出</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="卷积-conv"><a class="markdownIt-Anchor" href="#卷积-conv"></a> 卷积 Conv</h2>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>out</mtext><mo stretchy="false">(</mo><msub><mi>N</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>C</mi><msub><mtext>out</mtext><mi>j</mi></msub></msub><mo stretchy="false">)</mo><mo>=</mo><mtext>bias</mtext><mo stretchy="false">(</mo><msub><mi>C</mi><msub><mtext>out</mtext><mi>j</mi></msub></msub><mo stretchy="false">)</mo><mo>+</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>C</mi><mtext>in</mtext></msub><mo>−</mo><mn>1</mn></mrow></munderover><mtext>weight</mtext><mo stretchy="false">(</mo><msub><mi>C</mi><msub><mtext>out</mtext><mi>j</mi></msub></msub><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>⋆</mo><mtext>input</mtext><mo stretchy="false">(</mo><msub><mi>N</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0973199999999999em;vertical-align:-0.34731999999999996em;"></span><span class="mord text"><span class="mord">out</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34731999999999996em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0973199999999999em;vertical-align:-0.34731999999999996em;"></span><span class="mord text"><span class="mord">bias</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34731999999999996em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.1415490000000004em;vertical-align:-1.302113em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.839436em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.311105em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3340428571428572em;"><span style="top:-2.357em;margin-left:-0.07153em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">in</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.302113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">weight</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34731999999999996em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋆</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">input</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>卷积动画页面：<a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">conv_arithmetic/README.md at master · vdumoulin/conv_arithmetic</a></li>
</ul>
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>含义</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>in_channels</strong></td>
<td>输入通道数</td>
<td>彩色图通常为 3 (RGB)，灰度图为 1。</td>
</tr>
<tr>
<td><strong>out_channels</strong></td>
<td>输出通道数</td>
<td>卷积核的数量。有多少个核，输出就有多少层特征图。</td>
</tr>
<tr>
<td><strong>kernel_size</strong></td>
<td>卷积核大小</td>
<td>提取特征的“窗口”大小。常用 3 或 5（VGG 常用 3）。</td>
</tr>
<tr>
<td><strong>stride</strong></td>
<td>步长</td>
<td>窗口滑动的跨度。默认为 1。步长越大，输出图片越小。</td>
</tr>
<tr>
<td><strong>padding</strong></td>
<td>填充</td>
<td>在图片四周补 0。<code>'same'</code> 保持大小不变，<code>'valid'</code> 则不补。</td>
</tr>
<tr>
<td><strong>dilation</strong></td>
<td>空洞卷积</td>
<td>卷积核点之间的间距。用于扩大感受野（不用增加参数量）。</td>
</tr>
<tr>
<td><strong>bias</strong></td>
<td>偏置</td>
<td>是否在结果上加一个常数偏移。默认开启。</td>
</tr>
</tbody>
</table>
<ul>
<li>卷积的Padding（边界扩充）参数很重要，如果周围不补零，那么卷积会导致图像尺寸越来越小</li>
<li>形状计算公式：</li>
</ul>
<p class='katex-block katex-error' title='ParseError: KaTeX parse error: Expected &#039;}&#039;, got &#039;_&#039; at position 131: …s (\text{kernel_̲size}[0] - 1) -…'>H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
                        \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>计算同理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train = <span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">mymodule = myModule() <span class="comment"># 模型实例化 </span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = mymodule(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="comment"># torch.Size([64, 3, 32, 32])</span></span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    <span class="comment"># torch.Size([64, 6, 30, 30]) channel == 6 卷积后，channel数变化，不能直接输出图像</span></span><br><span class="line"></span><br><span class="line">    step = step + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<hr />
<h2 id="最大池化-maxpool"><a class="markdownIt-Anchor" href="#最大池化-maxpool"></a> （最大）池化 MaxPool</h2>
<ul>
<li>最大池化的逻辑非常简单：在一个窗口（Kernel）范围内，<strong>只保留最大的那个值</strong>，剩下的全部扔掉</li>
<li>既保留输入特征，又减小了数据量，加快训练速度</li>
</ul>
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>独特之处</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>kernel_size</strong></td>
<td>窗口大小。常见是 <code>2</code>（即将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span> 的区域合并）。</td>
</tr>
<tr>
<td><strong>stride</strong></td>
<td><strong>默认值等于 kernel_size</strong>！这和卷积不同。如果 <code>kernel_size=2</code>，步长默认就是 <code>2</code>，这样窗口之间就不会重叠。</td>
</tr>
<tr>
<td><strong>ceil_mode</strong></td>
<td><strong>非常重要。</strong> 默认是 <code>False</code>（向下取整）。如果设为 <code>True</code>（向上取整），当窗口超出边界时，只要窗口内有数据，就会保留结果，而不是舍弃。</td>
</tr>
<tr>
<td><strong>padding</strong></td>
<td>填充。注意池化填充的是<strong>负无穷（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">∞</span></span></span></span>）</strong>，这样是为了保证填充位不会被选为最大值。</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                     [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]], dtype=<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>,(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myMoudle</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(myMoudle, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>) </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        ceil_mode = False 就表示只有当池化核遇到的尺寸是最大尺寸（如3x3）时</span></span><br><span class="line"><span class="string">        才会取其池化的结果，否则相反</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="variable language_">self</span>.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">mymoudle = myMoudle()</span><br><span class="line">output = mymoudle(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">tensor([[[[<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">          [<span class="number">5.</span>, <span class="number">1.</span>]]]], dtype=torch.float64)</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="损失函数与反向传播"><a class="markdownIt-Anchor" href="#损失函数与反向传播"></a> 损失函数与反向传播</h2>
<ul>
<li>损失函数用于计算实际输出与目标之间的差距，为反向传播、更新参数提供一定的依据。在分类任务中，常用交叉熵函数来计算误差，</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss_func = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h3 id="优化器"><a class="markdownIt-Anchor" href="#优化器"></a> 优化器</h3>
<p><a target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/optim.html">torch.optim — PyTorch 2.10 documentation</a></p>
<ul>
<li>示例代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output, target) <span class="comment"># 调用损失函数</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step() </span><br></pre></td></tr></table></figure>
<h2 id="pytorch实战cifar10"><a class="markdownIt-Anchor" href="#pytorch实战cifar10"></a> Pytorch实战：CIFAR10</h2>
<ul>
<li>针对CIFAR10图像数据集的简单分类模型实战</li>
</ul>
<img src="/2026/01/22/pytorch/Screenshot_2026-01-26_140919.png" class="">
<ul>
<li>首先了解Sequential，<code>nn.Sequential</code> 是 <code>nn.Module</code> 的一个特殊子类，它的作用是<strong>自动完成 <code>forward</code> 逻辑</strong>。注意：其中每一个参数都是某层的类，所以要写逗号。Sequential既简化了模型定义，也简化了<code>forward()</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CIFAR10_Simple</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(CIFAR10_Simple, <span class="variable language_">self</span>).__init__(*args, **kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        参数padding的数值可以从想象图像得到：5x5的卷积核，当中心在图像的(0,0)，那么卷积核是扩展出去2格的。上面是简单判断的方法，实际应该用尺寸计算公式来代入计算（参考“Conv卷积”节内容）</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.model_s = Sequential(</span><br><span class="line">            Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        x = self.conv1(x)</span></span><br><span class="line"><span class="string">        x = self.maxpool1(x)</span></span><br><span class="line"><span class="string">        x = self.conv2(x)</span></span><br><span class="line"><span class="string">        x = self.maxpool2(x)</span></span><br><span class="line"><span class="string">        x = self.conv3(x)</span></span><br><span class="line"><span class="string">        x = self.maxpool3(x)</span></span><br><span class="line"><span class="string">        x = self.flatten(x)</span></span><br><span class="line"><span class="string">        x = self.linear1(x)</span></span><br><span class="line"><span class="string">        x = self.linear2(x)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x = <span class="variable language_">self</span>.model_s(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>在搭建模型前，先设置<code>DataLoader</code>处理数据集</p>
<ul>
<li>分别设置好<code>train_data</code>和<code>test_data</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset_transfrom = tf.Compose([tf.ToTensor(),tf.Normalize((<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>),(<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>))])</span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, transform=dataset_transfrom, download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=dataset_transfrom, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># --------</span></span><br><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>搭建好模型后，简单测试输出尺寸是否符合要求</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cifar = CIFAR10_Simple()</span><br><span class="line"><span class="built_in">print</span>(cifar)</span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)) <span class="comment"># 同数据集图片尺寸的测试</span></span><br><span class="line">output = cifar(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
<ul>
<li>训练前的基本设置
<ul>
<li>定义<code>TensorBoard</code>的<code>writer</code></li>
<li>设置<code>device</code>调用显卡加速训练</li>
<li>实例化训练模型</li>
<li>定义损失函数</li>
<li>设置优化器</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line">writer.add_graph(cifar, <span class="built_in">input</span>)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = CIFAR10_Simple().to(device)</span><br><span class="line">loss_func = nn.CrossEntropyLoss() <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">optim = torch.optim.SGD(cifar.parameters(), lr=<span class="number">0.01</span>) <span class="comment"># 学习率</span></span><br></pre></td></tr></table></figure>
<ul>
<li>训练部分
<ul>
<li>优化器的定式代码</li>
<li>记录训练损失</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total_step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># --- 训练部分 ---</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = model(imgs.to(device))</span><br><span class="line">        loss = loss_func(outputs, targets.to(device))</span><br><span class="line">        </span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optim.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 记录训练损失</span></span><br><span class="line">        writer.add_scalar(<span class="string">&quot;Train_Loss&quot;</span>, loss.item(), total_step)</span><br><span class="line">        total_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>评估部分
<ul>
<li>每个epoch执行一次</li>
<li><code>with torch.no_grad()</code>：关闭梯度记录</li>
<li>统计性能指标</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 测试时不需要计算梯度，节省性能</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            imgs, targets = imgs.to(device), targets.to(device)</span><br><span class="line">            outputs = model(imgs)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算总损失</span></span><br><span class="line">            loss = loss_func(outputs, targets)</span><br><span class="line">            total_test_loss += loss.item()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算准确率：argmax(1) 找到概率最大的类别索引</span></span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy += accuracy</span><br></pre></td></tr></table></figure>
<ul>
<li>可视化</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 输出到 TensorBoard</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;Test_Loss&quot;</span>, total_test_loss / <span class="built_in">len</span>(test_loader), epoch)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;Test_Accuracy&quot;</span>, total_accuracy / <span class="built_in">len</span>(test_data), epoch)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span> 结束，准确率: <span class="subst">&#123;total_accuracy / <span class="built_in">len</span>(test_data)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://ottercoconut.github.io">Kawauso</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章連結: </span><span class="post-copyright-info"><a href="https://ottercoconut.github.io/2026/01/22/pytorch/">https://ottercoconut.github.io/2026/01/22/pytorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版權聲明: </span><span class="post-copyright-info">除特別聲明外，本博客所有文章均採用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 授權協議。轉載請註明出處：<a href="https://ottercoconut.github.io">边个濑椰のBlog</a>。</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/TensorBoard/">TensorBoard</a><a class="post-meta__tags" href="/tags/Transforms/">Transforms</a><a class="post-meta__tags" href="/tags/Dataset/">Dataset</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.gif" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2026/01/28/cs224n/" title="cs224n"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一頁</div><div class="info-item-2">cs224n</div></div><div class="info-2"><div class="info-item-1"> Intro 本篇是对Stanford CS 224N | Natural Language Processing with Deep Learning (Spring 2024)这门课程的学习笔记。关于这门课的内容，总结如下：  词向量、RNN、LSTM、Seq2Seq 模型、机器翻译、注意力机制、Transformer 等等   What is this course about? Natural language processing (NLP) or computational linguistics is one of the most important technologies of the information age. Applications of NLP are everywhere because people communicate almost everything in language: web search, advertising, emails, customer service, language translation, virtual...</div></div></div></a><a class="pagination-related" href="/2026/01/15/toefl-speaking/" title="TOEFL Speaking"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一頁</div><div class="info-item-2">TOEFL Speaking</div></div><div class="info-2"><div class="info-item-1"> TOEFL Speaking  Task 1  Template  State Your Main Point  “I agree with the idea that…” “I think it is better to…” “I think it is a great/terrible idea to…”   Transition to the Details  “I feel this way for two reasons.”   Support your Point  “First…” + “For example…” “Second…” + “To be more specific…”     Agree / Disagree Do you agree or disagree with the following statement? Children should help their parents with household chores as soon as they are old enough. Use specific reasons and exa...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.gif" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Kawauso</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">標籤</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分類</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ottercoconut"><i class="fab fa-github"></i><span>喵~</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">关注边个濑椰谢谢喵~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目錄</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#dataset"><span class="toc-number">1.</span> <span class="toc-text"> Dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89dataset"><span class="toc-number">1.1.</span> <span class="toc-text"> 自定义Dataset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorboard"><span class="toc-number">2.</span> <span class="toc-text"> TensorBoard</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#summarywriter"><span class="toc-number">2.1.</span> <span class="toc-text"> SummaryWriter</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#add_scalar"><span class="toc-number">2.1.1.</span> <span class="toc-text"> add_scalar()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#add_image"><span class="toc-number">2.1.2.</span> <span class="toc-text"> add_image()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#add_graph"><span class="toc-number">2.1.3.</span> <span class="toc-text"> add_graph()</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84transforms"><span class="toc-number">3.</span> <span class="toc-text"> 常见的Transforms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#totensor"><span class="toc-number">3.1.</span> <span class="toc-text"> ToTensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#normalize"><span class="toc-number">3.2.</span> <span class="toc-text"> Normalize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#resize"><span class="toc-number">3.3.</span> <span class="toc-text"> Resize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#compse"><span class="toc-number">3.4.</span> <span class="toc-text"> Compse</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text"> Pytorch数据集使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dataloader"><span class="toc-number">5.</span> <span class="toc-text"> DataLoader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nnmodule"><span class="toc-number">6.</span> <span class="toc-text"> nn.Module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF-conv"><span class="toc-number">7.</span> <span class="toc-text"> 卷积 Conv</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96-maxpool"><span class="toc-number">8.</span> <span class="toc-text"> （最大）池化 MaxPool</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">9.</span> <span class="toc-text"> 损失函数与反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">9.1.</span> <span class="toc-text"> 优化器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E5%AE%9E%E6%88%98cifar10"><span class="toc-number">10.</span> <span class="toc-text"> Pytorch实战：CIFAR10</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/30/wsl2-proxy/" title="如何在WSL2上使用本机代理">如何在WSL2上使用本机代理</a><time datetime="2026-01-30T08:31:20.000Z" title="發表於 2026-01-30 16:31:20">2026-01-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/28/cs224n/" title="cs224n">cs224n</a><time datetime="2026-01-28T06:31:08.000Z" title="發表於 2026-01-28 14:31:08">2026-01-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/22/pytorch/" title="Pytorch基础">Pytorch基础</a><time datetime="2026-01-22T02:49:28.000Z" title="發表於 2026-01-22 10:49:28">2026-01-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/15/toefl-speaking/" title="TOEFL Speaking">TOEFL Speaking</a><time datetime="2026-01-15T08:41:43.000Z" title="發表於 2026-01-15 16:41:43">2026-01-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/13/toefl-writing/" title="TOEFL Writing">TOEFL Writing</a><time datetime="2026-01-13T06:25:25.000Z" title="發表於 2026-01-13 14:25:25">2026-01-13</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Kawauso</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主題 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="閱讀模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="切換日夜模式"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="單欄與雙欄切換"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="設定"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目錄"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到頂部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>